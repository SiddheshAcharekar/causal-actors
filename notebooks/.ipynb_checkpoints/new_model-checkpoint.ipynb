{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Bernoulli\n",
    "from pyro.distributions import Delta\n",
    "from pyro.distributions import Normal\n",
    "from pyro.distributions import Uniform\n",
    "from pyro.distributions import LogNormal\n",
    "from pyro.infer import SVI\n",
    "from pyro.infer import Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import torch.distributions.constraints as constraints\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "# initialize the autodiagonal with init_to_feasible instead of init_to_median\n",
    "from pyro.infer.autoguide import init_to_feasible\n",
    "\n",
    "# Data Loader\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../box_office/')\n",
    "from data_loader import load_tensor_data\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensors, y_train_tensors, actors, full_data = load_tensor_data(\"../data/ohe_movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_z(params):\n",
    "    \"\"\"Samples from P(Z)\"\"\"    \n",
    "    z_mean0 = params['z_mean0']\n",
    "    z_std0 = params['z_std0']\n",
    "    z = pyro.sample(\"z\", Normal(loc = z_mean0, scale = z_std0))\n",
    "    return z\n",
    "\n",
    "def f_x(z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(X|Z)\n",
    "    \n",
    "    P(X|Z) is a Bernoulli with E(X|Z) = logistic(Z * W),\n",
    "    where W is a parameter (matrix).  In training the W is\n",
    "    hyperparameters of the W distribution are estimated such\n",
    "    that in P(X|Z), the elements of the vector of X are\n",
    "    conditionally independent of one another given Z.\n",
    "    \"\"\"\n",
    "    def sample_W():\n",
    "        \"\"\"\n",
    "        Sample the W matrix\n",
    "        \n",
    "        W is a parameter of P(X|Z) that is sampled from a Normal\n",
    "        with location and scale hyperparameters w_mean0 and w_std0\n",
    "        \"\"\"\n",
    "        w_mean0 = params['w_mean0']\n",
    "        w_std0 = params['w_std0']\n",
    "        W = pyro.sample(\"W\", Normal(loc = w_mean0, scale = w_std0))\n",
    "        return W\n",
    "    W = sample_W()\n",
    "    linear_exp = torch.matmul(z, W)\n",
    "    # sample x using the Bernoulli likelihood\n",
    "    x = pyro.sample(\"x\", Bernoulli(logits = linear_exp))\n",
    "    return x\n",
    "\n",
    "def f_y(x, z, params):\n",
    "    \"\"\"\n",
    "    Samples from P(Y|X, Z)\n",
    "    \n",
    "    Y is sampled from a Gaussian where the mean is an\n",
    "    affine combination of X and Z.  Bayesian linear\n",
    "    regression is used to estimate the parameters of\n",
    "    this affine transformation  function.  Use torch.nn.Module to create\n",
    "    the Bayesian linear regression component of the overall\n",
    "    model.\n",
    "    \"\"\"\n",
    "    predictors = torch.cat((x, z), 1)\n",
    "\n",
    "    w = pyro.sample('weight', Normal(params['weight_mean0'], params['weight_std0']))\n",
    "    b = pyro.sample('bias', Normal(params['bias_mean0'], params['bias_std0']))\n",
    "\n",
    "    y_hat = (w * predictors).sum(dim=1) + b\n",
    "    # variance of distribution centered around y\n",
    "    sigma = pyro.sample('sigma', Normal(params['sigma_mean0'], params['sigma_std0']))\n",
    "    with pyro.iarange('data', len(predictors)):\n",
    "        pyro.sample('y', LogNormal(y_hat, sigma))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params):\n",
    "    \"\"\"The full generative causal model\"\"\"\n",
    "    z = f_z(params)\n",
    "    x = f_x(z, params)\n",
    "    y = f_y(x, z, params)\n",
    "    return {'z': z, 'x': x, 'y': y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define guide function for fitting latent variable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_guide(params):\n",
    "    \"\"\"\n",
    "    Guide function for fitting P(Z) and P(X|Z) from data\n",
    "    \"\"\"\n",
    "    # Infer z hyperparams\n",
    "    qz_mean = pyro.param(\"qz_mean\", params['z_mean0'])\n",
    "    qz_stddv = pyro.param(\"qz_stddv\", params['z_std0'],\n",
    "                         constraint=constraints.positive)\n",
    "    \n",
    "    z = pyro.sample(\"z\", Normal(loc = qz_mean, scale = qz_stddv))\n",
    "    \n",
    "    # Infer w params\n",
    "    qw_mean = pyro.param(\"qw_mean\", params[\"w_mean0\"])\n",
    "    qw_stddv = pyro.param(\"qw_stddv\", params[\"w_std0\"],\n",
    "                          constraint=constraints.positive)\n",
    "    w = pyro.sample(\"w\", Normal(loc = qw_mean, scale = qw_stddv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define guide function for fitting Bayesian Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_2_guide(params):\n",
    "    # Z and W are just sampled using param values optimized in previous step\n",
    "    z = pyro.sample(\"z\", Normal(loc = params['qz_mean'], scale = params['qz_stddv']))\n",
    "    w = pyro.sample(\"w\", Normal(loc = params['qw_mean'], scale = params['qw_stddv']))\n",
    "    \n",
    "    # Infer regression params\n",
    "    # parameters of (w : weight)\n",
    "    w_loc = pyro.param('w_loc', params['weight_mean0'])\n",
    "    w_scale = pyro.param('w_scale', params['weight_std0'])\n",
    "\n",
    "    # parameters of (b : bias)\n",
    "    b_loc = pyro.param('b_loc', params['bias_mean0'])\n",
    "    b_scale = pyro.param('b_scale', params['bias_std0'])\n",
    "    # parameters of (sigma)\n",
    "    sigma_loc = pyro.param('sigma_loc', params['sigma_mean0'])\n",
    "    sigma_scale = pyro.param('sigma_scale', params['sigma_std0'])\n",
    "\n",
    "    # sample (w, b, sigma)\n",
    "    w = pyro.sample('weight', Normal(w_loc, w_scale))\n",
    "    b = pyro.sample('bias', Normal(b_loc, b_scale))\n",
    "    sigma = pyro.sample('sigma', Normal(sigma_loc, sigma_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Latent Variable Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step_1(x_data, params):\n",
    "    \n",
    "    adam_params = {\"lr\": 0.0005}\n",
    "    optimizer = Adam(adam_params)\n",
    "\n",
    "    conditioned_on_x = pyro.condition(model, data = {\"x\" : x_data})\n",
    "    svi = SVI(conditioned_on_x, step_1_guide, optimizer, loss=Trace_ELBO())\n",
    "    \n",
    "    print(\"\\n Training Z marginal and W parameter marginal...\")\n",
    "\n",
    "    n_steps = 2000\n",
    "    pyro.set_rng_seed(101)\n",
    "    # do gradient steps\n",
    "    pyro.get_param_store().clear()\n",
    "    for step in range(n_steps):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "            \n",
    "    # grab the learned variational parameters\n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter{}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "        \n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step_2(x_data, y_data, params):\n",
    "    print(\"Training Bayesian regression parameters...\")\n",
    "    pyro.set_rng_seed(101)\n",
    "    num_iterations = 1000\n",
    "    pyro.clear_param_store()\n",
    "    # Create a regression model\n",
    "    optim = Adam({\"lr\": 0.003})\n",
    "    conditioned_on_x_and_y = pyro.condition(model, data = {\n",
    "        \"x\": x_data,\n",
    "        \"y\" : y_data\n",
    "    })\n",
    "\n",
    "    svi = SVI(conditioned_on_x_and_y, step_2_guide, optim, loss=Trace_ELBO(), num_samples=1000)\n",
    "    for step in range(num_iterations):\n",
    "        loss = svi.step(params)\n",
    "        if step % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step + 1, loss/len(x_data)))\n",
    "    \n",
    "    \n",
    "    updated_params = {k: v for k, v in params.items()}\n",
    "    for name, value in pyro.get_param_store().items():\n",
    "        print(\"Updating value of hypermeter: {}\".format(name))\n",
    "        updated_params[name] = value.detach()\n",
    "    print(\"Training complete.\")\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated function to train entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    num_datapoints, data_dim = x_train_tensors.shape\n",
    "    \n",
    "    latent_dim = 30 # can be changed\n",
    "#     print(torch.zeros(data_dim + latent_dim).shape)\n",
    "    params0 = {\n",
    "        'z_mean0': torch.zeros([num_datapoints, latent_dim]),\n",
    "        'z_std0' : torch.ones([num_datapoints, latent_dim]),\n",
    "        'w_mean0' : torch.zeros([latent_dim, data_dim]),\n",
    "        'w_std0' : torch.ones([latent_dim, data_dim]),\n",
    "        'weight_mean0': torch.zeros(data_dim + latent_dim),\n",
    "        'weight_std0': torch.ones(data_dim + latent_dim),\n",
    "        'bias_mean0': torch.tensor(0.),\n",
    "        'bias_std0': torch.tensor(1.),\n",
    "        'sigma_mean0' : torch.tensor(1.),\n",
    "        'sigma_std0' : torch.tensor(0.05)\n",
    "    }\n",
    "\n",
    "    params1 = training_step_1(x_train_tensors, params0)\n",
    "    params2 = training_step_2(x_train_tensors, y_train_tensors, params1)\n",
    "    return params1, params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Z marginal and W parameter marginal...\n",
      "[iteration 0001] loss: 304.3461\n",
      "[iteration 0101] loss: 294.8547\n",
      "[iteration 0201] loss: 290.4372\n",
      "[iteration 0301] loss: 281.5974\n",
      "[iteration 0401] loss: 274.5142\n",
      "[iteration 0501] loss: 273.1243\n",
      "[iteration 0601] loss: 261.1506\n",
      "[iteration 0701] loss: 260.4133\n",
      "[iteration 0801] loss: 250.2013\n",
      "[iteration 0901] loss: 248.1334\n",
      "[iteration 1001] loss: 250.6025\n",
      "[iteration 1101] loss: 246.0507\n",
      "[iteration 1201] loss: 240.1931\n",
      "[iteration 1301] loss: 232.8412\n",
      "[iteration 1401] loss: 229.0232\n",
      "[iteration 1501] loss: 215.9541\n",
      "[iteration 1601] loss: 209.8044\n",
      "[iteration 1701] loss: 201.3092\n",
      "[iteration 1801] loss: 187.6613\n",
      "[iteration 1901] loss: 183.0304\n",
      "Updating value of hypermeterqz_mean\n",
      "Updating value of hypermeterqz_stddv\n",
      "Updating value of hypermeterqw_mean\n",
      "Updating value of hypermeterqw_stddv\n",
      "Training Bayesian regression parameters...\n",
      "[iteration 0001] loss: 258.9689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyro\\infer\\trace_elbo.py:135: UserWarning: Encountered NaN: loss\n",
      "  warn_if_nan(loss, \"loss\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0101] loss: 263.9321\n",
      "[iteration 0201] loss: 222.0467\n",
      "[iteration 0301] loss: 206.6688\n",
      "[iteration 0401] loss: 199.0078\n",
      "[iteration 0501] loss: 194.0477\n",
      "[iteration 0601] loss: 178.9697\n",
      "[iteration 0701] loss: 181.7087\n",
      "[iteration 0801] loss: 179.2907\n",
      "[iteration 0901] loss: 178.9970\n",
      "Updating value of hypermeter: w_loc\n",
      "Updating value of hypermeter: w_scale\n",
      "Updating value of hypermeter: b_loc\n",
      "Updating value of hypermeter: b_scale\n",
      "Updating value of hypermeter: sigma_loc\n",
      "Updating value of hypermeter: sigma_scale\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# trained_params = train_model()\n",
    "p1, p2 = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save params to disk for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all params to disk\n",
    "import pickle\n",
    "\n",
    "with open('params.pickle', 'wb') as handle:\n",
    "    pickle.dump(p2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
